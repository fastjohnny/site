USE CTRL+F For Search

<hr />

<pre>Basis
To avoid interface name changes via udev (pass options to kernel): 
net.ifnames=1 biosdevname=1

Check json from page (chrome)
F12 -&gt; Network -&gt; Preserver log

Using rsync with append -
rsync --append-verify SOURCE DEST

ASCII with CRLF terminators, removing CRLF:
vim filename
:set fileformat=unix
:wq

Add version: 2.0.0 in deployment_tasks via VIM:
%s/^\s*role: .*/  version: 2.0.0\r&amp;/g

Check missed packages on interfaces:
 ip -s -s link show 

Debug resource start via pcs
pcs resource debug-start

check buffers:
ethtool -g 

set up maximum buffer for Nic:
 ethtool -G  rx 

add this change to script - (RHEL)
1. vim   /etc/sysconfig/network-scripts/ifcfg-NIC
2. add line:
  ETHTOOL_OPTS="-G  rx &lt;размер буфера&gt;"

Changes to Tx buffer are not recommended because of devices on other side of link

Check crm disk status  (full disk):
`crm node status-attr  show "#health_disk"`
delete flag: 

Recursive sed replacement in files:
find . -type f -exec sed -i 's/foo/bar/g' {} +

Recursive Disk Usage
du -h --max-depth=1 /var/log | sort -hr
WIFI connection password:
/etc/NetworkManager/system-connections/conn_name.conf 
[wifi security]
psk = 

Creating patches:
diff -Naur oldfile newfile &gt;new-patch

Disabling cloud-init datasource search 
`echo 'datasource_list: [ None ]' | sudo -s tee /etc/cloud/cloud.cfg.d/90_dpkg.cfg`Disabling service via update-rc.d:
update-rc.d drbd disable

Enabling corosync autostart
vim /etc/default/corosync - change NO on YES

Using deprecated branches 
git checkout kilo-eol 

Submitting existing repository in new gerrit repo:
   git clone $repo-from-stash
   cd $repo
   git checkout $branches #creating branches
   git remote add gerrit $gerrit-repo
   for i in {1000..1}; do echo $i; git reset --hard HEAD~$i; if [[ $? == 0 ]]; then break; fi; done #searching for first commit for squashing
   git merge --squash HEAD@{1}    #squashing
   git commit
   git pull gerrit $branch     #fetching repo to common ancestor
   git-review #Voila!
#repeat all steps for all branches

Sed all files in dir 
find . -type f -print0 | xargs -0 sed -i 's/str1/str2/g' 

FC scsi bus rescan: 
echo 1 &gt; /sys/class/fc_host/host#/issue_lip 
or apt install sg3_utils SCSI bus rescan: 
echo “- - -” &gt; /sys/class/scsi_host/hostH/scan 

or (working on fuel/vsphere):
[root@fuel2 ~]#  ls /sys/class/scsi_device/ 
2:0:0:0  3:0:0:0 
[root@fuel2 ~]# echo 1 &gt; /sys/class/scsi_device/2\:0\:0\:0/device/rescan  
[root@fuel2 ~]# echo 1 &gt; /sys/class/scsi_device/3\:0\:0\:0/device/rescan

Drop vm cache: 
echo 3 &gt; /proc/sys/vm/drop_caches 

Identify wwn of fc card: 
cat /sys/class/fc_host/hostX/device/fc_host/hostX/node_name 

Из какого пакета file.name
dpkg -S file.name 

rpm -qi --filesbypkg $package

QCOW2 to VMDK openstack convertation: 
qemu-img create -O vmdk -o adapter_type=lsilogic,compat6 ubuntu.qcow2 ubuntu.vmdk 

How to avoid drbd "start mounting"/"start daemon" race condition: 
vim /etc/network/if-up.d/drbd-start 
#! /bin/sh 
/etc/init.d/drbd start 
vim /etc/fstab 
/dev/drbd1 /home ext3 _netdev,relatime 0 2 

Install simple VM via virsh
qemu-img create -f qcow2 /var/lib/libvirt/images/cloud-linux.img 15G 
virt-install --connect qemu:///system --hvm --name cloud-linux --ram 1548 --vcpus 1 --cdrom ПУТЬ К ОБРАЗУ.ISO --disk path=/var/lib/libvirt/images/cloud-linux.img,format=qcow2,bus=virtio,cache=none --network network=default,model=virtio --memballoon model=virtio --vnc --os-type=linux --accelerate --noapic --keymap=en-us --video=cirrus --force</pre>
<pre> Ceph debugging
1. ceph -s ( Check your active flags (like norecovery, nobackflip, etc...))
2. ceph osd tree
3. ceph health detail (| grep blocked)
4. telnet monitor : 6789 (on ctrl node)
5. status ceph-osd id=$id
6. ceph pg dump | grep stuck

</pre>
<pre> Mcollective
Some mcollective useful commands:
mco rpc -v excurte_shell_command execute cmd="shotgun2 short-report" -I master
</pre>
<pre> Cisco 

Monitoring in console:
term mon

Check link:
on server: ifconfig interface up
on switch: show status int eth1/1

Check interface status: show interface status
Grep: | include ....
Check config: show run conf | inc

cdp vpc
1. enable cdp
2. enable vpc
3. create vpc domain
4. configure keepalive dest mgmt-ip-peer source mgmt-ip

Cisco LACP:
1.create portchannel group LACP mode active
2. feature lacp
TRUNK
2. configure if-range
3. spanning tree
4. assign if-range to portchannel
Example:
eth1809-3: conf t
eth1809-3(config): interface port-channel 1
eth1809-3(config): negotiate auto
eth1809-3(config): vpc 1
eth1809-3(config): switchport mode trunk
eth1809-3(config): exit
eth1809-3(config): interface eth1/1
eth1809-3(config): no sh
eth1809-3(config): switchport mode trunk
eth1809-3(config): channel-group 1 mode active
eth1809-3(config): exit
eth1809-3(config): wr

Enable Jumbo frames:
cz-eth1809-3(config)# policy-map type network-qos jumbo
cz-eth1809-3(config-pmap-nq)#   class type network-qos class-default
cz-eth1809-3(config-pmap-nq-c)#           mtu 9216
cz-eth1809-3(config-pmap-nq-c)# system qos
cz-eth1809-3(config-sys-qos)#   service-policy type network-qos jumbo

</pre>
<pre>Vmware/vsphere

Port security configure -
1. Hosts
2. Host
3. Configure
4. Networking
5. vswitch edit
6. Disable all security checkbox

Disconnected /deactivated datastore:
Check your license first
</pre>
<pre> Influxdb openstack access 
1. find astute.yaml, and values  influxdb_dbname, influxdb_username, influxdb_userpass и в секции vips&gt;influxdb&gt;ipaddr
2. access database: influx -host IPADDR -username INFLUXDB_USERNAME -password INFLUXDB_USERPASS -database INFLUXDB_DBNAME
3. SHOW MEASUREMENTS
4. SHOW FIELD KEYS FROM virt_memory_total
</pre>
<pre> Influx cache maximum memory size exceeded #6109
In logs:
14:52:07 reading file /data1/influxdb/wal/sysnoc/default/2/_00703.wal, size 10504926 [cacheloader] 2016/03/24 14:52:09 reading file /data1/influxdb/wal/sysnoc/default/2/_00704.wal, size 10494123 run: open server: open tsdb store: [shard 2] cache maximum memory size exceeded

Solution: 
1. vim tsdb/engine/tsm1/cache.go
   @@ -306,6 +306,12 @@ func (c *Cache) Delete(keys []string) {
       }
   }

+func (c *Cache) SetMaxSize(size uint64) {
+    c.mu.Lock()
+    c.maxSize = size
+    c.mu.Unlock()
+}
+
2. vim tsdb/engine/tsm1/engine.go
 @@ -659,6 +659,14 @@ func (e *Engine) reloadCache() error {
         return err
     }

+    limit := e.Cache.MaxSize()
+    defer func() {
+        e.Cache.SetMaxSize(limit)
+    }()
+
+    // Disable the max size during loading
+    e.Cache.SetMaxSize(0)
+
     loader := NewCacheLoader(files)
3. restart influxdb

</pre>
<pre>Haproxy
Haproxy ssl
Enabling tls on grafana:
scp kibana.lma.mos.cloud.sbrf.ru.chain.pem kaira6:/var/lib/astute/haproxy/
vim /var/lib/astute/haproxy/kibana......

bind 10.127.32.37:80
bind 10.127.32.37:443 ssl crt /var/lib/astute/haproxy/kibana.lma.mos.cloud.sbrf.ru.chain.pem no-sslv3 no-tls-tickets ciphers AES128+EECDH:AES128+EDH:AES256+EECDH:AES256+EDH
balance source
option forwardfor
option httplog
reqadd X-Forwarded-Proto:\ https
stick on src
stick-table type ip size 200k expire 30m
timeout client 3h
timeout server 3h

Grafana:
(haprxy conf)

bind 10.127.32.37:80
bind 10.127.32.37:443 ssl crt /var/lib/astute/haproxy/grafana.lma.mos.cloud.sbrf.ru.chain.pem no-sslv3 no-tls-tickets ciphers AES128+EECDH:AES128+EDH:AES256+EECDH:AES256+EDH
balance source
Удалить строчку
option forwardfor
option httplog
option httpchk GET /login/ HTTP/1.0
reqadd X-Forwarded-Proto:\ https
stick on src
stick-table type ip size 200k expire 30m
</pre>

<hr />

<pre>Haproxy 

1. If haproxy cannot start on all nodes after deployment ('cannot bind soc' after /usr/lib/ocf/resource.d/fuel/ns_haproxy reload), check this nonlocal_bind bug:
https://bugs.launchpad.net/fuel/+bug/1659205

Solution:
ip netns delete haproxy
/usr/lib/ocf/resource.d/fuel/ns_haproxy start
crm resource cleanup clone_p_haproxy

</pre>

<hr />

<pre>SSL

Ssl root ca cert add - mv cacert.crt /etc/ssl/certs/  

Check a certificate
openssl x509 -in certificate.crt -text -noout
or
openssl x509 -noout - hash - in cacert.crt

Openstack ca-bundle right order (hosttelecon):
1. crt -    
Issuer: C=GB, ST=Greater Manchester, L=Salford, O=COMODO CA Limited, CN=COMODO RSA Domain Validation Secure Server CA
Subject: OU=Domain Control Validated, OU=EssentialSSL Wildcard, CN=*.atlex.cloud
2. Bundle: 
2.1 -
Issuer: C=GB, ST=Greater Manchester, L=Salford, O=COMODO CA Limited, CN=COMODO RSA Certification Authority
Subject: C=GB, ST=Greater Manchester, L=Salford, O=COMODO CA Limited, CN=COMODO RSA Domain Validation Secure Server CA
2.2 - 
Issuer: C=SE, O=AddTrust AB, OU=AddTrust External TTP Network, CN=AddTrust External CA Root
Subject: C=GB, ST=Greater Manchester, L=Salford, O=COMODO CA Limited, CN=COMODO RSA Certification Authority
3. RSA Key

SSL Sparc Oracle
Verify: openssl x509 -noout -text -in Elbonia_Root_CA.pem
CHmod: chmod a+r Elbonia_Root_CA.pem
cp -p Elbonia_Root_CA.pem /etc/certs/CA/
Insert the cert in the end of file /etc/certs/ca-certificates.crt Hashed- link: /usr/sbin/svcadm restart /system/ca-certificates

Fuel postdeploy ca add:
1. Add cert in UI
2. fuel node --node $i --tasks upload_configuration
3. fuel node --node $id --tasks $ssl_tasks $haproxy-service-tasks $keystone-service-tasks
4. Profit 

ln -s cacert.crt ****e15.0
c_rehash . ; update-ca-certificates
Загвоздка с сертификатами, почему curl может работать, а клиенты нет.
Python-requests смотрит /etc/ssl/certs/ , но после установки certify смотрит
/usr/lib/python2.7/dist-objecti/certifi/bundle 
Этот bundle не содержит самоподписанные, поэтому после установки certify все ломается
Может помочь дописывание своего сертификата в ca_certificates.crt
Возможно, цепочка неправильная - открытая часть не должна присутствовать.

</pre>
<pre>LDAP

ldap 2 less
ldapsearch -x -LLL -h ******.ru -p 3268 -D openstack_ldap_user@******.ru -w 'D**2%$**&amp;amp;X5(' -b DC=ti,DC=local -s sub "(sAMAccountName=a.sh*****)" -P 3 -e ! chaining=referralsRequired
ldap on 2 less - ldapsearch -x -LLL -h 127.0.0.1 -p 389 -D cn=administrator,dc=local,dc=ru -w BNkmv/OEt+z1su_g_A_p_q_PjO6uA1C1 -b dc=******,dc=ru -s sub "(sAMAccountName=a.*****_ev)"</pre>

<hr />

<pre>Python using ssl verify cert -

<span class="kwd">import</span><span class="pln"> requests
</span><span class="pln">url </span><span class="pun">=</span> <span class="str">'https://foo.com/bar'</span><span class="pln">
r </span><span class="pun">=</span><span class="pln"> requests</span><span class="pun">.</span><span class="pln">post</span><span class="pun">(</span><span class="pln">url</span><span class="pun">,</span><span class="pln"> verify='/path/to/ca'</span><span class="pun">)
</span>
* * *</pre>
<pre class="lang-py prettyprint prettyprinted">Zabbix / mariadb-mysql

1. Изменить кол-во соединений (персистент) - vi /etc/my.cnf.d/server.cnf
2. Изменить кол-во соединений (перманент) - mysql -u root -p; show variables like 'max_connections';
set global max_connections = 1000;
3. Если не помогло, смотреть /usr/lib/systemd/system/mariadb.service, т.е.
   sudo vi /etc/systemd/system/mariadb.service:
     .include /lib/systemd/system/mariadb.service
     [Service] 
     LimitNOFILE=10000
     LimitMEMLOCK=infinity
3. Таймауты от агентов - vi /etc/zabbix/zabbix-server.conf; Timeout 30;
4. Poller busy - vi /etc/zabbix/zabbix-server.conf; StartPollers=200;
5. Ручная интеграция - установка агентов на нодах (через wget), положить скрипты в /etc/zabbix/scripts;
в /etc/zabbix/zabbix-agent.conf вписать Server; ServerActive, Userparameters; На сервере включить автообноружение агентов,
импортировать шаблоны,залинковать их с нодами.
Подробнее, про ручную установку - 
[http://rklimenko.rocks/2016/05/05/zabbix-tips/](http://rklimenko.rocks/2016/05/05/zabbix-tips/)</pre>

<hr />

<pre class="lang-py prettyprint prettyprinted">Fuel master, docker

Change fuelmenu settings - run bootstrap_admin_node.sh

1. dockerctl list
2. dockerctl check
3. dockerctl restart &lt;contain.name&gt;
4. dockerctl backup / restore

# Delete all containers
docker rm $(docker ps -a -q)
# Delete all images
docker rmi $(docker images -q)

Kernel update to 4.4 for Fuel 9.0:
cp fuel_bootstrap_cli.yaml fuel_bootstrap_cli.yaml.bak
sed -i -e 's/generic-lts-trusty/generic-lts-xenial/g' \
  -e '/-[[:blank:]]*hpsa-dkms$/d' fuel_bootstrap_cli.yaml
fuel-bootstrap build --activate --label bootstrap-kernel44

Fixing fake disks issue on discover nodes:
(unsquah /squah active bootstrap, and add new line with Container)
https://git.openstack.org/cgit/openstack/fuel-nailgun-agent/commit/?id=13fb4009d3f7c46222791bb9623cb05f8ba42ad8

Plugin sync and graph
vim /var/www/nailgun/plugins/plugin_name/...
fuel plugins --sync
fuel graph --env env_id | grep task

Fuel disable/enable plugins for removing from env:
fuel --env 1 settings -d 
fuel --env 1 settings -u -f 

Fuel rsync library
fuel node --node 1 --start rsync_core_puppet --end plugins_rsync

Puppet dir for plugins on master-node (7):
/var/www/nailgun/plugins

Removing plugin with 'syntax error near fi' error:
rpm -e --noscripts $package_name
If there no active connection to nailgun in gui (7.0)
and docker cannot check status of containers due to empty pass creds, do:
1. cp /etc/fuel/astute.yaml.old /etc/fuel/astute.yaml 
2. dockerctl check all

Controller+compute fix:
fuel role --rel 2  --role controller  --file 1.yaml
vi 1.yaml
Conflicts: []

fuel role --rel 2  --update --file 1.yaml
In UI network l3 enable DVR

Change hostnames for all nodes:
fuel node | grep -E "^(\ )*[0-9]" | awk '{print $1,$5}' | while read id hostname; do fuel node --node-id $id --hostname $hostname; done 

fuel graph --download file.dot
open this .dot in OmniGraffle Or GraphViz

Bug:
0. Mcollective fail (last_run execution puppetd, yaml):
fix - delete 
/var/lib/puppet/state/last_run_summary.yaml
/var/lib/puppet/state/last_run_report.yaml

1. 8th Fuel often cannot config rabbits container at start, and dont waiting for him. DO: restart puppet apply in rabbitmq container:
rabbit apply .... nailgun/examples/rabbitmq_only.pp
2. You MUST setup correct pxe settings  DURING deployment process of master node. To do so, you need choose standart installation and after post-installation scripts and reboot press SPACEBAR when you will be prompted to 'press a key'
3. If you want advanced installation you MUST specify your primary mac adress in bootloader option (press TAB while selecting advanced option and change XX:XX:XX:XX:XX on your mac)
4. PXEe/admin network MUST have native vlan or nodes will not bootstraped through ubuntu_bootstrap image (cobbler:/var/lib/tftpboot/images/ubuntu_bootstrap)
5. NO dots '.' in hostnames of controllers - or rabbitmq will fail (NX domain error)
6. NO small partitions ( ~64 mb) in fuel node config
7. After every reset of environment disks configuration also resets
8. After 3 attemps of deploing task (puppet) task will fail, but deploying process will go further
9. Dont use Qemu hypervisor - use KVM instead
10. If you using local repo, u must run `fuel-mirror apply -G mos -P ubuntu` - or cloud-init module mcollective will not start and provisiong will fail
11.  Another possible error after 100% provisiong and node reboot -
if you are using boot-volume it have 2 disks, at least. one small (64mb)
 and one primary. If your provisioning fails maybe its because cloud-init 
cannot find it data_source on primary disk(beacuse its on small one). 
In this case, use this workaround on nodes on discover phase:
dd if=/dev/zero of=/dev/vdb bs=1M count=64 
12. If you have more than one nodegroup you should check accordance between nodegroups and group_id of nodes or your deployment will like failed with non-understandable error in nailgun like '24' '22' 'gateway' and so.
</pre>

<hr />

<pre class="lang-py prettyprint prettyprinted">Elasticsearch and LMA

Influx, access and query:
---
grep timestamp /var/log/influx.log
log in influx
use lma;
Select .... from ... as as; 

# check cluster status
curl localhost:9200/_cluster/health

# figure out which indices are in trouble
curl 'localhost:9200/_cluster/health?level=indices&amp;pretty'

# figure out what shard is the problem
curl localhost:9200/_cat/shards

#query to all objects
curl elastic_vip:9200/_all/compute.instance.exists/_search?
pretty=true&amp;size=10000

#query to specific object (events_2017-06-26)
curl -XGET “$ES_URL/events_2017-06-26/snapshot.create/_search?pretty” 

#query indices
curl ‘localhost:9200/_cat/indices?v’

#query mappings 
curl -XGET “$ES_URL/events_2017-06-26/_mapping?pretty”

#lma_diagnostic
bash: lma_diagnostics

#check buffer:
heka-cat -offset=48175879 /var/cache/log_collector/output_queue/elasticsearch_output/0.log | head -n 30

#to recover ES index, you can try:
curl http://ES:9200/log-2016-11-15/_flush?force
#then run
curl -XPOST 'http://ES:9200/_cluster/reroute'

#checking logs
ls -l /var/cache/log_collector/output_queue/elasticsearch_output/
cat /var/cache/log_collector/output_queue/elasticsearch_output/checkpoint.txt - check if its value changes
tail -n 30 /var/log/log_collector.log

#about idle packets messages:
this kind of messages (idle pack) are not critical as long as they change over the time (numbers of pack per plugin) .. here this could mean that message are "waiting" for some time to let processing other messages (backpressure in heka terminology)

#increasing poolsize  can help in case of idle packets
vim /etc/log_collector/global.toml
increase poolsize to 200 
crm resource restart p_clone-log_collector
</pre>

<hr />

<pre class="lang-py prettyprint prettyprinted">Rally
Установка на одну ноду в venv.

1.  wget -q -O- [https://raw.githubusercontent.com/openstack/rally/master/install_rally.sh](https://raw.githubusercontent.com/openstack/rally/master/install_rally.sh)
2.  chmod +x install_rally.sh
3.  ./install_rally.sh -d venv
4.  vi venv/samples/deployments/existing.json
5.  rally deployment create --filename venv/samples/deployments/existing.json --name luxof

If you have multi region environment: 
1) change two lines (search "region") here on hard-coded name of  your region  lib/python2.7/site-packages/rally/osclients.py#L196 (and if you installed rally in venv in /venv/lib/python.....)
2) Also change line in neutron return in osclients.py: 
L357 client = neutron.Client(self.......... endpoint_override=self._get_endpoint(service_type)) 

In case of multiple networks:
for every failed task do this:
1) vim /venv/src/sample/tasks/scenarios/nova/boot-and-delete.yaml
2) args:
        flavor:....
        image:.....
        nics: [{"net-id": "id"}]

Customize certification tests:
1.  vi venv/src/certification/openstack/task_arguments.yaml:

service_list:
- authentication
- nova
- neutron
- keystone
- cinder
- glance
use_existing_users: false
image_name: "^(cirros.*uec|TestVM)$"
flavor_name: "m1.tiny"
glance_image_location: "/root/cirros-0.3.4-i386-disk.img" (Image должен лежать там)
smoke: false
users_amount: 30 (этот аргумент и последующие можно поставить в 1ку для тестового прохода)
tenants_amount: 10
controllers_amount: 3
compute_amount: 3
storage_amount: 4
network_amount: 3

2.  vi venv/lib/python2.7/site-packages/rally/plugins/openstack/scenarios/nova/utils.py:
line 112: def _boot_server(self, image_id, flavor_id,

auto_assign_nic=True, **kwargs)

3.  vi src/certification/openstack/scenario/cinder.yaml:

line 162: CinderVolumes.create_nested_snapshots_and_attach_volume:

args:
nested_level: 5

### Rally Start:

rally task start rally.git/rally/certification/openstack/task.yaml --task-args-file rally.git/rally/certification/openstack/task_arguments.yaml

</pre>

<hr />

<pre class="lang-py prettyprint prettyprinted">Ceph Osd Rbd
**Разрешить клиентам-компьютам писать в пул**
ceph auth caps client.compute osd 'allow class-read object_prefix rbd_children, allow rwx pool=volumes, allow rwx pool=images, allow rwx pool=compute' mon 'allow r’

ceph noin:
ceph osd set noin

ceph log per osd:
ceph daemon osd.0 log dump

ceph log per osd grep slowest recent ops:
ceph daemon osd.0 dump_historic_ops
*utlilization of ceph*
ceph osd reweight-by-utilization 115 - normal utilization is 120%
average_util*120 = % drive full osd

Ceph fio instance testing:
fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --runtime=120 --readwrite=randrw --rwmixread=75 --size=15G

**Загрузить образ в ceph**
rbd --pool images ls -l
rados put {object-name} {file-path} --pool=data
rbd -p images –image-format 2 import cirros-0.3.0-x86-64-disk.img.1 $(uuidgen)
rados lspools&gt;

**ISCSI маппинг из rbd (для vmware и прочих) через tgt**
apt-get install tgt
tgtadm --lld iscsi --mode system --op show
(‘rbd’ should appear in “Backing stores:” if your tgtd supports rbd.)
rbd create iscsi-image --size 50000
tgtadm --lld iscsi --mode target --op new --tid 1 --targetname rbd
tgtadm --lld iscsi --mode logicalunit --op new --tid 1 --lun 1 --backing-store iscsi-image --bstype rbd
tgtadm --lld iscsi --op bind --mode target --tid 1 -I ALL

**ISCSI connect to node
**iscsiadm -m discovery -t st -p $IP
iscsiadm -m node --login
iscsiadm -m node -u
</pre>
<pre>Hyper-v Openstack integration win 2012 r2
-nova-compute

1.  Install on windows node https://cloudbase.it/installing-openstack-nova-compute-on-hyper-v/
2.  Configs - c:/program files(x86)/Cloudbase/Openstack/Nova/etc/nova.conf neutron.conf:
         [glance] api_servers=http://endpoint
         [neutron] url=endpoint; admin_tenant = serviceS; enable_security_groups=true
         [oslo_messaging] rabbit_host - br_ex ip
!! IMPORTANT: enable rabbit listening on 0.0.0.0 on controller node (vi /etc/rabbit/rabbit-env) - restart !!
3.  Upload vhd images to glance:
         qemu-img convert ubuntu12x64.min -O vpc -o subformat=dynamic ubuntu12x64.vhd
         glance image-create --container-format bare --disk-format vhd --name 'sample' --file 'ubuntu12x64.vhd'
4.  Check hypervisors in Openstack (horizon - admin - hypervisors, disable all hypervisors, except hyper-v on woindows - for testin)
5.  Check logs on windows machine - C:/Openstack/Logs/
6. https://ashwaniashwin.wordpress.com/2014/06/27/configure-remote-desktop-connection-to-hyper-v-virtual-machine/

-neutron (vxlan)

1. We need to create 2 interfaces on VM - first interf in br-ex net and vlan trunk as second.
2. Make NIC team (VLAN ID=private network id) from second adapter - https://blogs.technet.microsoft.com/keithmayer/2012/11/20/vlan-tricks-with-nics-teaming-hyper-v-in-windows-server-2012/
3. Disable firewall. After that you must have connectivity on both interfaces (in br-mesh and br-ex nets)
4. Install ovs agent for windows - https://cloudbase.it/open-vswitch-24-on-hyperv-part-1/
5. Make ovs settings (br-tun) as on compute nodes
6. Add "_" to c:/program files/cloudbase/openstack/nova/python2.7/site-packages/hyperv/neutron/hyperv_neutron_agent.py
https://git.openstack.org/cgit/openstack/networking-hyperv/commit/?id=8bc5a0352379cddc57c618ff745cee301b403b66
VLAN
If you want vlan connectivity between hyperv node and openstack you should use neutron-hyperv-agent.
Pass steps from 1 to 3 and your environment will be ready.</pre>
<pre> Openstack contrail - vm problems
Problem: No ssh connection to vm or bad net on compute node
Solution: 
ethtool -K eth0 tx off
ethtool -K eth1 tx off
ethtool -K bond0 tx off
ethtool -k bond0
(after reboot it disappers, so you need install new cron-
@reboot ethtool -K eth0 tx off )
</pre>
<pre>Hp snmp hardware monitoring
1. apt-get install snmp snmpd
2. iptables -I INPUT 1 -p udp --dport 161 -m comment --comment "snmp port" -j ACCEPT
3. iptables -I INPUT 1 -p tcp --dport 161 -m comment --comment "snmp port" -j ACCEPT
4. vi /etc/snmp/snmpd.conf 
   agentAddress udp:161
   #view - commented
5. vi /etc/apt/sources.list.d/hp.list
   deb http://downloads.linux.hpe.com/SDR/downloads/MCP/ubuntu precise current/non-free
6. apt-get update
7. apt-get download libc6-i386=2.19-0ubuntu6.7
8. dpkg -i libc6-i386_2.19-0ubuntu6.7_amd64.deb
9. apt-get install hp-health
10. wget http://downloads.linux.hpe.com/SDR/downloads/MCP/ubuntu/pool/non-free/hp-snmp-agents_10.0.0.1.23-20._amd64.deb
11. dpkg -i hp-snmp-agents_10.0.0.1.23-20._amd64.deb
12. /sbin/hpsnmpconfig
13. service snmpd restart
14. snmpwalk -v1 -c public localhost 1.3.6.1.4.1.232
15. zabbix template - hp_snmp_agents

</pre>
<pre> Supermicro snmp hardware monitoring
1. U'll need SuperDoctor  5 (x64) - 
http://www.supermicro.com/solutions/SMS_SD5.cfm
2. Also, you will need java 1.8 (jdk, preferable -
 just download java from oracle site, 
unzip and write `export PATH=PATH:/opt/java/` to bashrc)
3. Scp superdoctor on all nodes
4. Run SuperDoctorInstaller x64
5. During installation, specify your jdk path -  /opt/java/jdk1.8/bin/java; 
Set other promts to default
6. apt-get install snmpd snmp;
7. vi /etc/snmp/snmpd.conf
pass  .1.3.6.1.4.1.10876  /opt/Supermicro/SuperDoctor5/libs/native/snmpagent
rwcommunity public 127.0.0.1
rocommunity readonly 127.0.0.1
rwcommunity  public 10.216.203.241
 trapsink     localhost public
Comment out all lines with 'view' 'access' and so.
8. service snmpd restart &amp;&amp; service sd5 restart.
9. SD5 is very buggy tool, so you'll maybe need another restart/reboot. 
Check logs, 
check /opt/Supermicro/SuperDoctor5/libs/native/snmpagent -n .1.3.6.1.4.1.10876; 
10. For hard drives monitoring im using custom bash script with zabbix agent 
in front. Example:
10.1 vi /etc/zabbix/scripts/check_drive.sh
 #!/bin/sh -f
PATH=$path:/bin:/usr/bin:/usr/ucb
REQ="$1"
#echo $REQ
udisks --show-info /dev/sda | grep FAIL &gt; /dev/null;
sda=$?
#echo "sda -$sda sdb -$sdb sdc -$sdc sdd -$sdd" &gt;&gt; /tmp/log
#echo "$RET"
case "$REQ" in
  sda) echo "$sda" ;;
esac
10.2 vi /etc/zabbix/zabbix_agentd.conf
#Drives
UserParameter=drive.status.sda,/etc/zabbix/scripts/check_drive.sh sda
10.3 service zabbix_agent restart
</pre>
<pre>  Zabbix haproxy default bug
vi /etc/zabbix/scripts/haproxy.sh
  "-v")
    OPER='value'
    IFS=$'.'
    QA=($2)
    unset IFS
    HAPX=${QA[0]}
    HASV=${QA[1]}
    ITEM=${QA[2]}
</pre>
<pre>Murano Bugs
MOS 9

1. vim /usr/lib/python2.7/dist-packages/murano/api/v1/catalog.py
2. search def get_ui
3. insert line `tempf.flush()`
</pre>
<pre> Mysql openstack
Grafana - 
create database grafana;
create user 'grafana'@'%' identified by 'grafana';
grant all privileges on grafana.* to 'grafana'@'%';
quit;

</pre>
<pre>Puppet notes

1.Default parameter values - Use with capitalized resource spec wihout title 
                Exec { path =&gt; ['/usr/bin', '/bin'] } 

2. Native mysql commands - use module "puppetlabs-mysql"
3. Variable in variable - for facter
  $mule = "ipaddress_${name}"
   $donkey = inline_template("&lt;%= scope.lookupvar(mule) %&gt;")
    notify { "Found interface $donkey":; }
4. Chain in conditional:
    $cinder_volume_exist = inline_template("&lt;% if File.exist?('/etc/init.d/cinder-volume') -%&gt;;true&lt;% end -%&gt;")
    cinder_config { 
                          .......
    }
    if $cinder_volume_exist == 'true' {
         exec {"service cinder restart"
                  command =&gt; "service cinder-volume restart",
         }
       Cinder_config &lt;||&gt; ~&gt; Exec['service cinder restart']
   }
5. Array var in resource declaration:
 define process_osd {

   exec { "Prepare OSD $name":
             command =&gt; "ceph-deploy --ceph-conf /root/ceph.conf osd prepare localhost:$name$arr_len"
        }
}
   process_osd { $dev : }

</pre>
<pre>NEUTRON provider network FUEL deployment

Get future deployment settings:
fuel deployment --env $env_id –default

Update template for each node (Put these sections to the end of template):
vim deployment_3/*.yaml
transformations:
- action: add-br
name: br-private-phys
- action: add-br
name: br-private
provider: ovs
- action: add-patch
bridges:
- br-private
- br-private-phys
provider: ovs
mtu: 65000
- action: add-port
name: bond0
bridge: br-private-phys

Upload templates:
fuel deployment --env $env_id --upload
</pre>
<pre> Neutron tips 
In case of network unreachable in cloud-init -
crm resource restart p_clone_neutron_dhcp_agent
In case of connection refused - 
service neutron-l3-agent restart
</pre>
<pre> NEUTRON Provider network

3. accessing arrays - $arr[0]

4. hiera_hash - /etc/fuel/clsuter/id/astute.yaml

1. Configuring provider network

controller 

brctl addbr br-aux0
brctl addif br-aux0 eth1
ovs-vsctl add-br br-prv
ovs-vsctl add-port br-prv aux0-prv -- set Interface  aux0-prv type=internal
brctl addif br-aux0 aux0-prv
ifconfig br-aux0 up
ifconfig br-prv up
ifconfig aux0-prv up

compute

brctl addbr br-aux0
brctl addif br-aux0 eth2
ovs-vsctl add-br br-prv
ovs-vsctl add-port br-prv aux0-prv -- set Interface  aux0-prv type=internal
brctl addif br-aux0 aux0-prv
ifconfig br-aux0 up
ifconfig br-prv up
ifconfig aux0-prv up

nano /etc/network/interfaces.d/ifcfg-aux0-prv
iface aux0-prv inet manual
  ovs_bridge br-prv
  ovs_type OVSIntPort

nano /etc/network/interfaces.d/ifcfg-br-aux0
auto br-aux0
iface br-aux0 inet manual
bridge_ports eth2 aux0-prv

nano /etc/network/interfaces.d/ifcfg-br-prv
auto br-prv
allow-ovs br-prv
iface br-prv inet manual
ovs_ports aux0-prv
ovs_type OVSBridge
dfs

# nano -c /etc/neutron/plugin.ini
…
[ml2_type_vlan]
…
network_vlan_ranges = physnet1:33:63
[ovs]
...
bridge_mappings = physnet1:br-prv
...
### controller ###

primary-controller~# service neutron-server restart
primary-controller~# crm resource restart p_neutron-plugin-openvswitch-agent
primary-controller~# crm resource restart p_neutron-l3-agent

###compute###

compute~# service neutron-plugin-openvswitch-agent restart
compute~# service nova-compute restart

VERIFY:
#neutron net-create --provider:network_type=vlan \ 
--provider:physical_network=physnet1 --provider:segmentation_id 33 net33
</pre>
<pre> NEUTRON tips

1. Attaching fixed ip to vm:
       neutron port-create --tenant-id #tenant_id \ 
--fixed-ip subnet_id=#subnet_id,ip_address=*.*.*.*
       nova interface-attach --port-id #port_id #instance_id 
2. Adding static route to qrouter (ovs-vswitch)
       neutron router-update #router_id --routes type=dict list=true \ 
       destination=10.0.0.0/8,nexthop=10.1.3.1

</pre>
